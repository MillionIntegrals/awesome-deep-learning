# Awesome: Architecture 

### RNNs with Attention

- (Sep 2014) **Neural Machine Translation by Jointly Learning to Align and Translate**
  Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio
  https://arxiv.org/abs/1409.0473

### Transformers

Transformer implemented in PyTorch
- https://github.com/jadore801120/attention-is-all-you-need-pytorch

Blogposts:
- http://nlp.seas.harvard.edu/2018/04/03/attention.html
- https://blog.openai.com/language-unsupervised/
- https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/
- https://jalammar.github.io/illustrated-transformer/

- (Jun 2017) **Attention Is All You Need**
  Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin
  http://arxiv.org/abs/1706.03762
  
  
- (Jul 2018) **Universal Transformers**
  Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, ≈Åukasz Kaiser
  https://arxiv.org/abs/1807.03819
  
  
- (Jan 2019) **Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context**
  Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov
  https://arxiv.org/abs/1901.02860
